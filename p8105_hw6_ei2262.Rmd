---
title: "Homework 6"

output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(janitor)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


# Problem 2
```{r}
urlfile="https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
post_homicide = read_csv(url(urlfile))

set.seed(1)
post_homicide =
  post_homicide %>% 
  mutate(city_state = paste(city, state, sep=", ")) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(victim_race %in% c("Black", "White")) %>% 
  mutate(
    homicide_solved = as.numeric(disposition == "Closed by arrest"),
    victim_race = fct_relevel(victim_race, "White"),
    victim_age = as.numeric(victim_age),
    victim_sex = fct_relevel(victim_sex, "Female")
  ) %>% 
  select(city_state, homicide_solved, victim_age, victim_sex, victim_race)

post_homicide
```

### Odds Ratio and Confidence Interval for Baltimore, MD
```{r}
baltimore_reg =
  post_homicide %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(homicide_solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) %>%
  tidy() %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, std.error) %>% 
  mutate(
    odds_ratio = exp(estimate),
    low_conf = exp(estimate - 1.96*std.error),
    upper_conf = exp(estimate + 1.96*std.error)) %>% 
  select(-estimate, -std.error)
 
baltimore_reg %>% 
  knitr::kable(digits = 3)
```
Homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. 

### Odds Ratio and Confidence Interval for All Cities
```{r}
cities_reg = 
  post_homicide %>%
  nest(data = -city_state) %>% 
  mutate(model = map(data, ~glm(homicide_solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())),
         results = map(model, broom::tidy)) %>% 
  select(-data, -model) %>% 
  unnest(results) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, term, estimate, std.error) %>% 
  mutate(
    odds_ratio = exp(estimate),
    low_conf = exp(estimate - 1.96*std.error),
    upper_conf = exp(estimate + 1.96*std.error)) %>% 
  select(-estimate, -std.error)

cities_reg %>% 
  knitr::kable(digits = 3)
```

#### Plot
```{r}
cities_reg %>% 
  mutate(city_state = fct_reorder(city_state, odds_ratio)) %>% 
  ggplot(aes(x=city_state, y = odds_ratio)) +
  geom_point(stat = "identity", shape = 10) +
  geom_errorbar(aes(ymin = low_conf, ymax = upper_conf), width = 0.4) +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90)) +
  geom_hline(yintercept=1, color='black', linetype='dashed', alpha=.5)+
  labs(
    x = "City, State",
    y = "Odds Ratios"
    )
```
According to the plot, in most cities with significant odds ratios, homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female.  The cities in which the homicides in which the victim is male are significantly more likely to be resolved have a very wide confidence interval and include the null value (OR = 1). Therefore, the victim's sex may not have a significant difference on whether or not the homicide will be resolved. 

# Problem 3
```{r}
child_bwt = read_csv("data/birthweight.csv")

child_bwt %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  ) 
```

## Proposed Model 

The model I propose is a linear regression model with `bwt`, baby's birthweight (continuous, lbs), as the outcome and `wtgain`, mother's weight gain during pregnancy (continuous, lbs), as the main exposure. I hypothesize that a mother's weight at delivery as a positive association with baby's birthweight, meaning that a higher mother's weight at delivery is associated with a higher baby's birthweight. If the expecting mother gains a large amount of weight, there is a higher chance that the baby will be born weighing more than a baby born from a mother who does not gain much weight during her pregnancy. I hypothesize that `momage`, mother's age at delivery (continuous, years), `delwt`, mother's weight at delivery (continuous, lbs), and `fincome`, family monthly income (continuous, in hundreds, rounded) affect the association between baby's birthweight and mother's weight gain during pregnancy. Mother's age at delivery affects the association because the older a mother is at birth, the more at-risk she is. Therefore, there are many more factors that may contribute to her ability or need to gain weight during a pregnancy. A mother's weight at delivery affects the association because, although the mother might have gained a large amount of weight during her pregnancy, her weight at birth may not be that high. Family monthly income affects the association because a family's ability to afford groceries and other products that contribute to weight gain may affect the amount of weight a mother may gain during a pregnancy.

## Linear Regression Model Assessing the Association Between `bwt` (baby's birthweight, lbs) and `wtgain` (mother's weight gain during pregnancy, lbs)
```{r}
child_bwt %>% 
  mutate(
    bwt = bwt*0.0022046226
  )

bwt_linear = lm(bwt ~ wtgain + momage + delwt + fincome, data = child_bwt)

resids_fitted <-
  child_bwt %>% 
  add_residuals(bwt_linear, var = "resids") %>% 
  add_predictions(bwt_linear, var = "fitted_values") %>% 
  ggplot(aes(x = fitted_values, y = resids))+
  geom_point() +
  geom_line(color = "blue") +
  labs(
    title = "Residuals Against Fitted Values Plot",
    x = "Fitted Values",
    y = "Residuals"
  )

resids_fitted
```

## Compare Linear Models
```{r}
bwt_linear2 = lm(bwt ~ blength + gaweeks, data = child_bwt)

child_bwt %>% 
  mutate(
    bhead = bhead* 0.3937007874,
    blength = blength* 0.3937007874,
  )

bwt_linear3 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = child_bwt)
```
Models:

1. **Linear1** = Linear Model 1 (hypothesized by me)
    - Outcome: baby's birthweight (continuous, lbs)
    - Predictors: mother's weight gain (continuous, lbs), mother's age at delivery (continuous, years), family income (continuous, hundreds, rounded)
    
2. **Linear2** = Linear Model 2 (provided on homework)
    - Outcome: baby's birthweight (continuous, lbs)
    - Predictors: length at birth (continuous, inches), gestational age (continuous, weeks)

3. **Linear3**: Linear Model 3 (provided on homework)
    - Outcome: baby's birthweight (continuous, lbs)
    - Predictors: head circumference (continuous, inches), baby's length at birth (continuous, inches), baby's sex (binary), 2-way and 3-way interactions

## `crossv_mc`
```{r}
compare_cv = 
  crossv_mc(child_bwt, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    bwt_linear = map(train, ~lm(bwt ~ wtgain + momage + delwt + fincome, data = .x)),
    bwt_linear2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    bwt_linear3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_linear1 = map2_dbl(bwt_linear, test, ~rmse(model = .x, data = .y)),
    rmse_linear2 = map2_dbl(bwt_linear2, test, ~rmse(model = .x, data = .y)),
    rmse_linear3 = map2_dbl(bwt_linear3, test, ~rmse(model = .x, data = .y)))

compare_cv %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

#### Conclusion from Comparing 3 Linear Models

Based on the violin plot that was created, when my model (Linear1) is compared to the two other linear models, it is not the best model to use to understand the effects of several variables on a child's birthweight. It can be assumed that my model has little predictive value compared to model Linear2 and model Linear3. Out of the 3 models, Linear3 model has the best predictive value. Linear3 model may have the best predictive value because it incorporates interactions between the predictors in the model, which is a high possibility.  
